\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage[margin=3cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{CS 155}
\newcommand\semester{Spring 2014}  % <-- current semester
\newcommand\asgnname{HW 0}         % <-- assignment name
\newcommand\yourname{Charles Harrison}  % <-- your name
\newcommand\login{csharris}          % <-- your CS login

\newenvironment{answer}[1]{
  \subsubsection*{Problem #1}
}{\newpage}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\yourname\ (\login)\\\course\ --- \semester}
\chead{\textbf{\Large Homework \asgnname}}
\rhead{\today}
\headsep 10pt

\begin{document}

\begin{answer}{1.18}
$F : \{0, \ldots , n -1 \} \rightarrow \{0, \ldots, m-1\}$ where
$$F((x + y) \mod{n}) = (F(x) + F(y)) \mod{n}$$

The algorithm to computer $F(z)$ given a lookup table for $F$ with adversary altering $\frac{1}{5}$ of the entries is as follows:
\begin{verbatim}
function computeF(z):
    x = random.uniform(0, n - 1)
    y = z - x mod n
    return (F[x] + F[y]) mod m
\end{verbatim}

\begin{align*}
    Pr(\text{a given element of F is altered}) &= Pr(altered) = \frac{1}{5}\\
    Pr(\text{two uniformly random elements are altered}) &= Pr(altered \cup altered)\\
    &= Pr(altered) + Pr(altered)
    \intertext{because the probability of a union is the sum of the probabilities}
    &= \frac{1}{5} + \frac{1}{5} = \frac{2}{5}
\end{align*}
The algorithm will fail if either of the two elements chosen are altered. There is a $\frac{2}{5}$ chance that any two randomly chosen elements ($F[x]$ and $F[y]$) are altered. This means that our algorithm will succeed $1 - \frac{2}{5} = \frac{3}{5} > \frac{1}{2}$ of the time.
If we can run our initial algorithm 3 times, we can do the following:
\begin{verbatim}
    function enhancedComputeF(z):
        r1 = computeF(z)
        r2 = computeF(z)
        r3 = computeF(z)
        if no r values alike:
            return random choice of [r1, r2, r3]
        else:
            return mode([r1,r2,r3])
\end{verbatim}
We take the most occured value if multiple $r$ values are the same, and a random $r$ value if all are different.
\begin{align*}
    Pr(r = F[z]) &= Pr(\text{all correct} \cup \text{2 are correct} \cup \text{1 is correct and we pick it})\\
        &= Pr(\text{all correct}) + Pr(\text{2 are correct}) + Pr(\text{1 is correct and we pick it}) \\
    Pr(\text{all correct}) &=  Pr(\text{r1 correct} \cap \text{r2 correct} \cap \text{r3 correct})\\
        &= Pr(\text{r1 correct})*Pr(\text{r2 correct})*Pr(\text{r3 correct}) \text{ because all events are independent} \\
        &= \frac{3}{5}*\frac{3}{5}*\frac{3}{5} = \left(\frac{3}{5}\right)^3 = .216\\
    Pr(\text{2 correct}) &= {3 \choose 2}\left(\frac{3}{5}\right)^2 \left(\frac{2}{5}\right) = .432  \\
\end{align*}

$Pr(\text{1 is correct and we pick it})$ must be bounded, because it depends on how cruel our adversary is. There are two extremes:
    \begin{itemize}
        \item One cruel extreme is that the adversary alters each table value to be equal (i.e. $F[i] = F[j]$ for any altered $i$ and $j$). In this case any two wrong answers will be the same, and $Pr(\text{1 is correct and we pick it}) = 0$, because the mode will always return the value of the two equal wrong answers.
        \item Another extreme is that the adversary is nice and no two table values are the same, in this case the only way to get two wrong answers that are the same is to have the random indices be the same. For example, if we pick $x_1$ and $y_1$ and return $F[x_1] + F[y_1]$, then the only way a second call to computeF could return the same number (if it is wrong) is to have it pick $x_2$ and $y_2$ such that $x_2 = x_1$ or $y_2$ = $x_1$.
            $$$$
            So
            \begin{align*}
            Pr(\text{both wrong answers the same}) &= Pr(x_2 = x_1 \cup y_2 = x_1)
                &= Pr(x_2 = x_1) + Pr(y_2 = x_1)\\
                &= \frac{1}{n} + \frac{1}{n} = \frac{2}{n}\\
            Pr(\text{1 is correct and we pick it}) &= Pr(\text{1 is correct}) * Pr(\text{ we pick it})
                \intertext{ because the two events are completely independent}
                &= {3 \choose 1} \frac{3}{5} \left(\frac{2}{5}\right)^2 * Pr(\text{wrong answers differ $\cap$ we pick correctly}) \\
                &= .288 * Pr(\text{wrong answers differ})*Pr(\text{we pick correctly})
                \intertext{ we can separate the intersection because the events are independent}
                &= .288 * (1 - Pr(\text{both wrong answers are the same}) * \frac{1}{3}\\
                &= .288 * (1 - \frac{2}{n}) * \frac{1}{3} = 0.096(1 - \frac{2}{n})
            \end{align*}
    \end{itemize}
    So we can bound the probability that 1 of our computation is correct and we picked it by
        $$ 0 \le Pr(\text{1 correct and we picked it}) \le .096(1 - \frac{2}{n})$$
So all in all, using answers above:
\begin{align*}
    p &= P(r = F[z]) = P(\text{all correct}) + P(\text{2 are correct}) + P(\text{1 is correct and we pick it}) \\
     .216 + .432 + 0 &\le p \le .216 + .432 + .096(1 - \frac{2}{n}) \\
     .648 &\le p \le  .744 - \frac{.192}{n}
\end{align*}
However, it is important to note that if the adversary is not especially cruel (e.g. he is random), the probability we return the correct result is much closer to the upper bound.

\end{answer}



\begin{answer}{1.23}
Claim: there can be at most $\frac{n(n-1)}{2}$ distinct min-cut sets of a graph.\\
Proof: Recall that $P(\text{finding a specific min-cut}) = \frac{2}{n(n-1)}$. Furthermore, we can stipulate that $P(\text{finding any min-cut}) \le 1$. Let $k$ = the number of min-cut sets. Then

    \begin{align*} P(\text{finding any min-cut}) &= P(\text{finding } C_1 \cup \text{ finding } C_2 \cup \ldots \cup \text{ finding } C_k)\\
    &= P(\text{finding } C_1) + P(\text{finding } C_2) + \ldots + P(\text{finding } C_k)
    \intertext{ because we can always sum the probabilities of unioned events}
    &= k * P(\text{finding a specific } C_i) \\
    &= \frac{2k}{n(n-1)}\\
    &\le 1 \intertext{so}
    k &\le \frac{n(n-1)}{2}
    \end{align*}

\end{answer}




\begin{answer}{2.18}
\emph{Resevoir Sampling} is an algorithm that receives an input stream of data, and maintains a sample of one item with the property that it is uniformly distributed over all the items that we've seen so far. The algorithm works by storing only one element at a time, and when the $k$th item, $x_k$ appears, it replaces the item in memory with probability $\frac{1}{k}$.\\\\
Analysis: The algorithm correctly maintains the item with uniform distribution. That is, after $k$ steps, the element in memory, $r$, is equally likely to be any of $\{x_1, \ldots, x_k \}$.\\
Proof by Induction:
    \begin{itemize}
        \item Base case: $k = 1$, then $r = x_1$, and we have a uniform distribution over a single value.
        \item Inductive step: Assume the value of $r$ is uniformly distributed over all $k-1$ values in the stream, that is
            $$P(r = x_i) \text{ for } 1 < i < k = \frac{1}{k-1}$$
        When we read $x_k$, we replace it by $r$ with probability $\frac{1}{k}$ and keep the same value of $r$ with probability $\frac{k-1}{k}$. Thus at step $k$,
        \begin{itemize}
            \item $P(r = x_k) = \frac{1}{k}$
            \item $P(r = x_i) \text{ for } 0 < i < k =
                \left(\frac{1}{k-1}\right)\left(\frac{k-1}{k}\right) = \frac{1}{k}$
        \end{itemize}
        Therefore at step $k$ we have $r$ maintaining a uniform distribution over all $k$ items we've seen.
    \end{itemize}
    Because the base case holds, and the inductive step holds, the algorithm successfully maintains a uniform distribution over all seen items at every step in the stream.
\end{answer}




\begin{answer}{3.7}
For a stock with price starting price $1$ and $r > 1$, each day
\begin{itemize}
    \item $q' = qr$ with probability $p$
    \item $q' = q/r$ with probability $1 - p$
\end{itemize}
Let $Q$ be a random variable that takes on the final value of our stock, and $X$ be a random variable that takes on how many times our stock went up in $d$ days (how many good days). It is clear that $Q = 1*(r^X)*(r^{-(d - X)}) = r^{2X - d}$
\begin{enumerate}[a.]
\item Finding $E[Q]$. We know that $X$ is a binomial random variable, with expectation $dp$ and variance $dp(1-p)$. That is because the sample space of $X$ $\Omega$, are all the combinations of different runs of $d$ days we can have. Each day has two options, good day or bad day, so $|\Omega| = 2^d$.
Finally we can compute the following:
    \begin{align*}
        E[X] &= \sum_{s\in \Omega} Pr(X = s) X(s) \\
            &= \sum_{k = 0}^d {d \choose k}p^k (1 - p)^{d - k} * k = dp \text{ as expected}\\
        E[Q] &= E[r^{2X - d}] = \sum_{s\in \Omega} Pr(X = s) * r^{2X(s) - d}\\
            &= \sum_{k = 0}^d {d \choose k}\left( p^k * (1 - p)^{d - k} \right) r^{2k - d}\\
        E[Q] &= \left(\frac{ 1 + p(r^2 - 1) }{ r }\right)^d
    \end{align*}

\item Variance of the final stock price: $V[Q]$. Recall $V[Q] = E[Q^2] - E[Q]^2$. We can derive both of these from the information above
    \begin{align*}
    E[Q^2] &= \sum_{s \in \Omega} Pr(X = s) * (r^{2X(s) - d})^2 \\
        &= \sum_{k=0}^d {d \choose k}\left( p^k * (1 - p)^{d - k} \right) \left(r^{2k - d}\right)^2\\
        &= \left(\frac{p(r^4 - 1) + 1}{r^2}\right)^d\\
    E[Q]^2 &= \left(\frac{ 1 + p(r^2 - 1) }{ r }\right)^{2d}\\
    V[Q] = E[Q^2] - E[Q]^2 &= \left(\frac{p(r^4 - 1) + 1}{r^2}\right)^d - \left(\frac{ 1 + p(r^2 - 1) }{ r }\right)^{2d}
    \end{align*}
\end{enumerate}
\end{answer}
\end{document}

