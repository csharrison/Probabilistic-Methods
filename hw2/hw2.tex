\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage[margin=3cm]{geometry}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{CS 155}
\newcommand\semester{Spring 2014}  % <-- current semester
\newcommand\asgnname{HW 2}         % <-- assignment name
\newcommand\yourname{Charles Harrison}  % <-- your name
\newcommand\login{csharris}          % <-- your CS login

\newenvironment{answer}[1]{
  \subsubsection*{Problem #1}
}{\newpage}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{\yourname\ (\login)\\\course\ --- \semester}
\chead{\textbf{\Large Homework \asgnname}}
\rhead{\today}
\headsep 10pt

\begin{document}

\begin{answer}{4.3}
\begin{enumerate}[(a)]
	\item The moment generating function for the binomial random variable $B(n,p)$ is
		\begin{align*}
		M_{B(n,p)}(t) &= E[e^{B(n,p)t}] \\
			&= \sum_{k=0}^n {n \choose k} p^k(1-p)^{n-k} * e^{kt}\\
			&= (1 + p(e^t - 1))^n
		\end{align*}
	\item Let $X = B(n,p)$ and $Y = B(m, p)$ independent random variables. The moment generating function of $X + Y$ is
		\begin{align*}
		M_{X + Y}(t) &= M_X(t)M_Y(t) \text{ by theorem 4.3} \\
			&= (1 + p(e^t - 1))^n (1 + p(e^t - 1))^m \\
			&= (1 + p(e^t - 1))^{n + m}
		\end{align*}
	\item We can conclude from the form of $M_{X+Y}(t)$ that for any collection
		$$X_1 = B(n_1, p), X_2 = B(n_2, p), \ldots, X_k = B(n_k, p)$$

		 of binomial random variables with the same parameter $p$,

		 $$M_{X_1 + X_2 + \ldots + X_k}(t) = M_{B(n_1 + n_2 + \ldots + n_k, p)}(t)$$ Theorem 4.2 states that if $M_X(t) = M_Y(t)$, then $X = Y$. So therefore
		 $$B(n_1,p) + B_(n_2,p) + \ldots + B(n_k, p) = B(n_1 + n_2 + \ldots + n_k, p)$$
\end{enumerate}
\end{answer}





\begin{answer}{4.9}
	Suppose we obtain independent samples $X_1, X_2, \ldots$ of a random variable $X$ and we want to use these samples to estimate $E[X]$. Using $t$ samples, we use
		$$ \mu = \sum_{i=1}^t X_i$$
	for our estimate of $E[X]$. We want the estimate to be within $\epsilon E[X]$ from the true value of $E[X]$ with probability at least $1 - \delta$. \\
	Let $r = \frac{\sqrt{Var[X]}}{E[X]}$
\begin{enumerate}[(a)]
	\item Claim: $O\left( \frac{ r^2 }{ \epsilon^2 \delta }\right)$ samples are sufficient. \\ Proof:
		\begin{align*}
			E[\mu] &= E\left[\sum_{i=1}^t X_i\right] = t E[X_i] = t E[X] \\
			V[\mu] &= V\left[\sum_{i=1}^t X_i\right] = t V[X_i] = t V[X] \text{ by independence}
			\intertext{so $V[X] = V[\mu]/t$ and $E[X] = E[\mu]/t$, so use Chebyshev's}
			Pr\left( \mu/t - E[X] \ge  \epsilon E[X]\right) &= Pr\left( \mu - t E[X] \le  t\epsilon E[X]\right)\\
				&= Pr\left( \mu - E[\mu] \le  \epsilon E[\mu]\right) \\
				&\le \frac{V[\mu]}{\epsilon^2 E[\mu]^2} \\
				&= \frac{tV[X]}{\epsilon^2 t^2 E[X]^2}\\
				&= \frac{r^2}{t \epsilon^2}
			\intertext{so we can go back to our original claim, that we need}
			Pr\left( \mu/t - E[X] \le  \epsilon E[X]\right) &\ge 1 - \delta \\
			Pr\left( \mu/t - E[X] \le  \epsilon E[X]\right) &= 1 - Pr\left( \mu/t - E[X] \ge  \epsilon E[X]\right) \ge 1 - \delta\\
			P = Pr\left( \mu/t - E[X] \ge  \epsilon E[X]\right) &\le \delta
		\end{align*}
		So if $t = O(\frac{r^2}{\epsilon^2 \delta}) = k\frac{r^2}{\epsilon^2 \delta}$, then $P \le \frac{r^2}{t\epsilon^2}$ implies that $P \le \frac{r^2}{\epsilon^2}*\frac{\epsilon^2}{r^2}\frac{\delta}{k} = \frac{\delta}{k}$, so the claim holds.
	\item If we only need to be within $\epsilon E[X]$ of $E[X]$ with probability at least 3/4, then we only need $O(\frac{r^2}{\epsilon^2})$ samples. \\
	Proof: If our probability is at least 3/4, then it means that $1 - \delta = \frac{3}{4}$ so $\delta = \frac{1}{4}$. So then
		$$ O\left(\frac{r^2}{\epsilon^2 \delta}\right) = O\left(\frac{4r^2}{\epsilon^2}\right) = O\left(\frac{r^2}{\epsilon^2}\right)$$

	\item By taking the median of $n = O(log(\frac{1}{\delta})$ weak estimates, we can obtain an estimate within $\epsilon E[X]$ of $E[X]$ with probability at least $1 - \delta$. Therefore we only need $O\left(\frac{r^2log(1/\delta)}{\epsilon^2}\right) samples$ \\
	Proof: We know that if more than half of the estimates fall within our bounds then the median will also fall within our bounds. Define a random variable $X_i$
		$$X_i = \begin{cases}
			1 &\mbox{ if the $i$th weak estimate is above $\epsilon E[X]$ of $E[X]$}\\
			0 &\mbox{ otherwise (good case)}
		\end{cases}$$
	We know that $Pr(X_i = 0) = \frac{3}{4}$ and $Pr(X_i = 1) = \frac{1}{4}$ (these equalities are approximations). So each $X_i$ is binomial and independent.
		Let $X = \sum_{i = 1}^n X_i$, we can use a Chernoff bound on $X$! We know that $E[X] = n*E[X_i] = n * \frac{1}{4}$
		$$ Pr(X  \ge (1 + \alpha)E[X]) \le e^{-E[X]\delta^2 / 3}$$
		Take $\alpha = 1$
		so $Pr(X \ge \frac{n}{2}) \le e^{-n/12}$.
		\\
		So if we take $n = 12log(\frac{1}{\delta})$ then
		$$ P(X \ge \frac{n}{2}) \le \delta$$
		So using $n = 12log(\frac{1}{\delta})$, we can get that the probability more than half of the estimates fall outside our bounds is $\le \delta$, so the probability the median of the estimates satisfies our bounds is $\ge 1- \delta$.
		Each weak estimate uses $O(\frac{r^2}{\epsilon^2})$ samples and we take $n = O(log(1/\delta))$ samples, so our total samples used is
		$O(\frac{r^2 log(1/\delta)}{\epsilon^2}$

\end{enumerate}
\end{answer}


\begin{answer}{4.10}
	A slot machine costs \$1 to play and (independently) yields
		$$
		\begin{cases}
			3 &\mbox{ with probability } \frac{4}{25} \\
			100 &\mbox{ with probability } \frac{1}{200} \\
			0 &\mbox{ otherwise}
		\end{cases}
		$$
	The casino is surprised they lost $\$10000$ over the first million games. \\
	Let $X$ be a random variable taking on how much money the casino makes on the slot machine. Let $X_i$ be random variables taking on how much money the casino makes on the $i$th play. It is clear that
		\begin{align*}
			X_i &= \begin{cases}
					-99 &\mbox{ with probability } \frac{1}{200} \\
					-2  &\mbox{ with probability } \frac{4}{25} \\
					1  &\mbox{ with probability } 1 - \frac{1}{200} - \frac{4}{25} = \frac{167}{200}
				\end{cases}\\
			X &= \sum_{i = 1}^{1000000} X_i
			\intertext{We can derive the moment generating function of $X_i$ by}
			M_{X_i}(t) &= E[e^{X_it}]\\
				&= \frac{1}{200}e^{-99t} + \frac{4}{25}e^{-2t} + \frac{167}{200}e^t
			\intertext{Using theorem 4.3 we can say that}
			M_X(t) &= \prod_{i = 1}^{1000000} M_{X_i}(t)\\
				&= \left(M_{X_i}(t)\right)^{1000000}
			\intertext{Using Chernoff Bounds, we can say that}
			Pr(X \le a) &\le \min_{t < 0} \frac{E[e^{Xt}]}{e^{at}}\\
			Pr(X \le -10000) &\le \min_{t < 0} \frac{  1 } { e^{-10000t} } \left(\frac{1}{200}e^{-99t} + \frac{4}{25}e^{-2t} + \frac{167}{200}e^t\right)^{1000000}
			\intertext{Using a very powerful calculator, we find $t = -.0005779$ to yield the minimum, giving us}
			Pr(X \le -10000) &\le .000159
		\end{align*}
		So in reality the casino was pretty unlucky, most likely due to karma (or their machines were broken).
\end{answer}

\end{document}

